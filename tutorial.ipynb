{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCFemTech Tour de Code: Data Science & Machine Learning Tutorial\n",
    "\n",
    "### Tutorial Overview\n",
    "This tutorial is divided into 2 parts. During the first half of the tutorial, we introduce data cleaning and exploratory data analysis. The second half will cover more advanced topics such as Machine Learning.\n",
    "\n",
    "At any point during the tutorial if you get stuck or need further clarification, please do not hesitate to raise your hand so that a facilitator can help you.\n",
    "\n",
    "### Dataset\n",
    "The dataset `census.csv`  was derived from the Census Bureau Database and is a comprehensive record of over 48,000 individuals and their socio-economic information.\n",
    "\n",
    "### The Challenge\n",
    "Determine a person's income level based on the socio-economic based on the socio-economic measures given.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Cleaning & Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "<font color=blue> Please install the necessary Python packages before starting this notebook. The packages used are:\n",
    "- pandas (as pd)\n",
    "- numpy (as np)\n",
    "- datetime (as dt)\n",
    "- plotly (plotly.plotly as py / plotly.graph_objs as go)\n",
    "- matplotlib.pyplot (as plt)\n",
    "- seaborn (as sns)\n",
    "    \n",
    "</font>\n",
    "\n",
    "#### STEP 1: IMPORT THE PACKAGES\n",
    "Import the packages we'll be using during the exercise. This has already been done for you. Just select the cell and click \"Run\" (or Shift+Return on Macbooks) to execute the package installation. \n",
    "\n",
    "Remember, if you're using Anaconda, you'll need to create a new kernel (follow the instructions in the participant's guide)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This are the packages you need. Simply run this cell.\n",
    "from IPython.display import display\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "plotly.offline.init_notebook_mode()\n",
    "from plotly.offline import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option('max_rows', 999) # default is 60 rows\n",
    "\n",
    "#Always set a random seed to replicate your results\n",
    "np.random.seed(44)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have your packages installed, we will load our census data from our csv (comma-separated value) file into a **Pandas DataFrame**. A dataframe is which is a 2-dimensional labeled data structure with columns of potentially different types. Each row has a unique label (the row index), and each column has a unique label (the column index). Simply stated, a dataframe is a table of data, similar to a spreadsheet or SQL table.\n",
    "\n",
    "A csv file is a text file containing data in table form, where columns are separated using the ‘,’ comma character, and rows are on separate lines. Loading a csv file is made extremely simple with the `.read_csv()` function in Pandas, once you know the path to your file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this line to \"read-in\" your dataset\n",
    "census = pd.read_csv(\"datasets/census.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've read in the data, use the `.head()` method, which prints the first 5 rows (by default). You can use this method to inspect just the beginning of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this to see the first 5 rows of your dataset\n",
    "census.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "census.head(30) # Prints the first 30 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to see the last rows of your dataset? Using the `.tail()` method, inspect the end of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use .tail() method to see the bottowm rows\n",
    "census.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `.columns` to get a list of all the column names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "census.columns\n",
    "#where did the () go? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like some colums have extra white space at the beginning or the end (race, income). Let's quickly remove these leading and trailing whitespaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove leading or trailing whitespace from columns:\n",
    "census.columns = census.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our column names won't work. We strongly recommend removing dashes and only using underscores\n",
    "census.columns = census.columns.str.replace('-', '_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.shape` command gives information on the size of the dataset. The first number is the number of rows and the second is the number of columns.\n",
    "\n",
    "#### Q. How many rows and columns are in the data set? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input number of rows and columns here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many dataframes have mixed data types, that is, some columns are integers, some are strings, and some are dates etc. Internally, csv files do not contain information on what data types are contained in each column. Pandas infers the data types when loading the data.\n",
    "\n",
    "Use the `.dtypes` method to check the types of each column.\n",
    "\n",
    "*Note: strings are loaded as ‘object’ datatypes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the .dtypes method here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `.describe()`, `.unique()`, and `.info()` to do more basic exploration before we begin cleaning the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter .describe(), .unique() and .info() here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "census.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select a column of data using the name of the column as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How to choose a column/series:\n",
    "census['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In pandas, you can also use a . to select a column/series\n",
    "census.age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census[['age']] # Selects index and column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census[['age','sex', 'education']] # Selects multiple columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try it yourself. \n",
    "#### Q. Print the occupation column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print occupation column:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q. Print the marital status, relationship, and age columns below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Marital status column/series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relationship column/series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Age column/series:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data\n",
    "Perfect data sets are rare. There are advanced techniques for cleaning messy data sets. The primary goal should be to tidy the data. The two principals of tidy data are as follows:\n",
    "* Each column represents a variable. Each row represents an observation.\n",
    "* Similar data grouped together is a dataset.\n",
    "\n",
    "### Missing Value(s)\n",
    "Are there any missing data? Any outliers? Shown here we see that the oldest person is **90** years old. Does that make sense? What other areas can you check to see where data is missing or not statistically representative? \n",
    "\n",
    "Cleanse the data by removing outlying observations with spurious or erronous data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data cleaning here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the number of nulls per column you can use the `.isnull()` and `.sum()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_sum = census.isnull().sum() #sum of all the nulls\n",
    "null_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also visualize this by plotting the number of nulls per column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How to make a bar graph. And how to plot null values\n",
    "\n",
    "null_sum.plot(kind='bar') # Specifies a bar graph\n",
    "plt.title('Number of null values per column') # Creates a title for the graph\n",
    "plt.show() # Displays the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have identified outliers and missing data, let's try removing them. Pandas uses the `.drop()` function to remove rows and columns. \n",
    "\n",
    "Let's first try removing a column. To delete a column, use the name of the column, and specify the “axis” as 1 (rows are axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the column titled \"native-country\" and preview the first 5 row.\n",
    "census.drop(\"native_country\", axis=1).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try removing some row. To delete a row, use the index labels, and specify the \"axis\" as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the rows with label 2 and preview the first 5 rows.\n",
    "census.drop([2], axis=0).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how if you print the dataframe, the previously deleted column and row are still there! \n",
    "\n",
    "Take a minute and discuss this with the person sitting next to you as to why this could be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census.head()\n",
    "#census.shape  #uncomment census.shape to see how many rows and columns there are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to remove rows which contain missing values, we can use the `.dropna()` function. We can also specify whether we want to drop a row if it contains all NA or at least one NA.\n",
    "\n",
    "* ‘any’ : If any NA values are present, drop that row or column.\n",
    "* ‘all’ : If all values are NA, drop that row or column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove missing values. Then look at the shape to see what was dropped\n",
    "census = census.dropna(axis=0, how=\"any\")\n",
    "census.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "Now that we have prepped the data by cleaning it, let's dive deeper into the data to see what we can find. \n",
    "\n",
    "### Summary Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use built-in functions to find various types of summary statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census.age.min() # Find the minimum age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census.age.max() # Find the maximum age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census.age.median() # Find the median age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census.age.mean() # Find the average age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Data\n",
    "\n",
    "We can use simple operator comparisons on columns to extract relevant or drop irrelevant information. If we want to filter the data to see only the individuals who are less than 20 years of age we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find people lesss than 20 years old \n",
    "age_filter = census.age < 20 \n",
    "census[age_filter] # Will filter our dataframe by the condition we specified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the dataframe to see only the individuals who work more than 40 hours per week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours_worked_filter = census[\"hours-per-week\"] > 40 #folks who work more than 40 hrs per week\n",
    "census[hours_worked_filter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try filtering for only females:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sex_filter = census.sex == \"Female\" #in Python == means equal. Know the diference between = and ==\n",
    "census[sex_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census[census.sex == \"Female\"] # Same as above cell, condensed onto one line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine multiple conditions as well by using the **&** operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census[(census.age < 20) & (census.sex == \"Female\")] #folks who are under age 20 and female"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it yourself. Work with the person next to you to create your own filter. Be prepared to share your filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create your own filter here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Exploratory Data Analysis (EDA)\n",
    "\n",
    "Often while working with Pandas dataframe you might have a column with categorical variables, string/characters, and you want to find the frequency counts of each unique elements present in the column. Pandas’ `.value_counts()` easily let you get the frequency counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find value counts here\n",
    "census['education'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q. How many folks have a 9th grade education?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.groupby()` method is a very powerful Pandas method. \n",
    "You can group by one column and count the values of another column per this column value using the `.value_counts()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provides a value count for marital-status by country\n",
    "census.groupby('native-country')['marital-status'].value_counts() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the groupby function in combination with statistical functions as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each gender, calculate the mean of the numeric columns in the dataframe.\n",
    "census.groupby('sex').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try grouping by age and find the mean of numeric columns in the dataframe. Do you notice any trends?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use groupby the age and find mean of numeric columns in dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOW IT'S YOUR TURN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn. Use the cheat sheets and your facilitators, as well as help from the other teams and try to answer some of the following questions or create your own questions\n",
    "\n",
    "* What is the min, max, mean, and median age?\n",
    "* How many individuals per country?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find the value counts of the native country.\n",
    "# You may be getting an error....let's explain\n",
    "census['native_country'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn:\n",
    "\n",
    "Now that you've walked through some of getting started with time series and geospatial analysis, spend time exploring the data and doing your own analysis. Consider what questions you'd like answered and begin answering them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Machine Learning and Predictive Analysis\n",
    "\n",
    "What kind of predictive analysis can we do on this data? Which countries have incomes below or above $50k? \n",
    "\n",
    "Machine learning is a technique that uses the past to guess at what the future will hold. We can apply it here in a variety of ways. There are two main types of machine learning: supervised (where the target is labeled) and unsupervised (where the target is not labeled). We'll work primarily with supervised machine learning for this data set. Beyond that, there are several main tasks for the machines: categorizing data based on it's features or predicting out the future.\n",
    "\n",
    "![Python's ML Package, Scikit-learn has it's own cheat sheet](http://scikit-learn.org/stable/_static/ml_map.png)\n",
    "http://scikit-learn.org/stable/_static/ml_map.png\n",
    "\n",
    "You may have seen while doing your exploratory analysis that some trends are clearer than others. There's always a fuzzy area and machine learnining works to handle that ambiguity in ways that humans are not so great at. Of course, there's plenty of legal and ethical questions surrounding the use of machine learning, as input biases lead to output biases and creating fairness in artificial intelligence is a hot topic, but for now we'll use machines to see if we can draw any new insights from out data.\n",
    "\n",
    "Here we'll study two different classifiers, and the methods for determining their accuracy. The first is Random Forest Classifier and the second is called a Support Vector Machine. They work in different methods which will be explained further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the splitter and the Machine Learning (ML) algorithms we will use.\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn import tree\n",
    "#from sklearn.cross_validation import KFold, cross_val_score\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing the Data\n",
    "\n",
    "Before going into machine learning, we have to do some exploratory data analysis to see what features in the machine make a difference to the outcome. Where there are poor correlations, we're likely to see poor performance, where as stronger correlations mean better performance. Below, we'll view the data two different ways -- using a pair plot and a correlation plot heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "census.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see the relationship of our target variable (income) to each other variable:\n",
    "sns.set(style=\"ticks\")\n",
    "sns.pairplot(census, hue=\"income\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's build a correlation matrix (.corr() method). \n",
    "#We need to know which features are/aren't correlated with each other. \n",
    "#The goal is to feed uncorrelated features into the model.\n",
    "\n",
    "corr = census.corr()\n",
    "sns.heatmap(corr, annot = True); #in seaborn, put a ; at the end to remove that funny line\n",
    "#annot = True means to place the values in each square."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "Some models only work with numeric data. That means we must convert all the words (categorical data) to numbers in order to feed it into our models. \n",
    "\n",
    "Let's get to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this line to \"read-in\" your dataset\n",
    "census = pd.read_csv(\"datasets/census.csv\")\n",
    "\n",
    "#Remove leading or trailing whitespace from columns:\n",
    "census.columns = census.columns.str.strip()\n",
    "\n",
    "#Our column names won't work. We strongly recommend removing dashes and only using underscores\n",
    "census.columns = census.columns.str.replace('-', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First take a look at your column names. You have 2 choices: rename them to remove the dashes or only use [] notation for columns\n",
    "census.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " data_frame = pd.get_dummies(data_frame, dummy_na=True, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's encode our columns. Let's make a dataframe for \n",
    "dummy_columns = census[['work_class', 'income', 'marital_status', 'race', 'sex', 'relationship', 'native_country', 'education', 'occupation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dummy_columns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "census.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's make a dataframe of our numeric columns. We'll merge this onto the dummy'd columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "numeric_columns = census[['age', 'fnlwgt', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's make magic in one line. \n",
    "#Here we concatenate the numeric columns to the dummied column, and drop the first one.\n",
    "df = pd.concat([numeric_columns, pd.get_dummies(dummy_columns, dummy_na=True, drop_first=True)], axis = 1)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['workclass_num', 'education_num', 'marital_num', 'race_num', 'sex_num', 'rel_num', 'capital.gain', 'capital.loss']]\n",
    "y = df.over50K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To feed our data into our model, we must define our X and y\n",
    "#X = our features - the useful variables we will use in our model\n",
    "#y = the target variable - the thing we are trying to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define your target variable. First.\n",
    "y = df['income_ >50K']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, define all of the features you'll use in your model. Always drop the y variable.\n",
    "X = df.drop(['income_ >50K', 'income_nan'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "This is one of the most commonly used models. Logistic regresssion allows us to predict classes. In this case, we are predicting whether or not income is greater than 50k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we must split our data into a training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's fit our model\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get a score! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Now let's score our model. Congratulations, you've built a model. This here is machine learning\n",
    "print(\"Training set accuracy: {:.3f}\".format(lr.score(X_train, y_train)))\n",
    "print(\"Test set accuracy: {:.3f}\".format(lr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One only predicts on the test set!\n",
    "y_pred_class = lr.predict(X_test)\n",
    "\n",
    "#Now let's put it into a dataframe so we can read the predictions next to the actual values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which features are more the most important in the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Logistic regression coefficients\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.plot(lr.coef_.T, 'o', label=\"C=1\")\n",
    "#plt.plot(logreg100.coef_.T, '^', label=\"C=100\")\n",
    "#plt.plot(logreg001.coef_.T, 'v', label=\"C=0.001\")\n",
    "plt.xticks(range(X.shape[1]), X, rotation=90)\n",
    "plt.hlines(0, 0, X.shape[1])\n",
    "#plt.ylim(-5, 5)\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Coefficient magnitude\")\n",
    "plt.legend()\n",
    "plt.savefig('log_coef')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "A decision tree is easy to understand -- if this/then this relationships are how most programming languages operate. The problem with such finite decisions is one of overfitting. It doesn't adjust for outlyers because a single tree trains until it's perfect on a single value. \n",
    "\n",
    "A random forest is a series of decision trees.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "\n",
    "#Now let's fit our model\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Now let's score our random forest model. Looks like this model is ... overfit (dun dun dun)\n",
    "print(\"Training set accuracy: {:.3f}\".format(rf.score(X_train, y_train)))\n",
    "print(\"Test set accuracy: {:.3f}\".format(rf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What features had the most difference in the training set?\n",
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn\n",
    "\n",
    "What can you get from the data here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn:\n",
    "\n",
    "Look into various implementations of tree models and give them a try. Do any improve the quality of the data? Also consider removing features or using different feature combinations to optimize your results. Can you get above an 80% accuracy? What happens when you change the test/train split? There are many parameters that can be changed when performing machine learning. Which ones give you the best results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Up\n",
    "\n",
    "What did you find during this exercise? What insights would you like to share with the world from doing this exploration? Here is your chance to make conclusions and present your findings. When you're finished, you may submit your kernel/notebook to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
